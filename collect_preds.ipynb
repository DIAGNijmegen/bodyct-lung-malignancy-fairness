{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from utilities import predictions\n",
    "from utilities.info import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Venk21 and PanCan Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venk21_pancan = pd.read_csv(f\"{INPUT_DIR}/NLST_DL_vs_PanCan_Venk21.csv\")\n",
    "venk21_pancan.rename(columns={'Diameter [mm]': 'Diameter_mm'}, inplace=True)\n",
    "venk21_pancan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venk21_pancan['timepoint'] = venk21_pancan['StudyDate'].map(lambda d: (d - 19990102) // 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venk21_pancan['DL_cal'] = predictions.calibrate_preds(venk21_pancan['DL'], venk21_pancan['label'])\n",
    "_= predictions.make_calibration_plots(venk21_pancan['label'], venk21_pancan['DL'], venk21_pancan['DL_cal'], title='\\nVenkadesh 2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = predictions.check_scoredists(venk21_pancan, ['DL', 'DL_cal', 'PanCan2b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Series for Sybil Predictions\n",
    "\n",
    "This includes the ones from Venk21's cross-validation set which are NOT in Sybil's training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_SYBIL_PICKLE_FILE = True\n",
    "\n",
    "if LOAD_SYBIL_PICKLE_FILE:\n",
    "    SYBIL_SERIES_JSON_PATH = f\"{INPUT_DIR}/sybil-nlst-info/sybil-nlst-pid_tp_series2split.p\"\n",
    "    sybil_split_dict = pickle.load(open(SYBIL_SERIES_JSON_PATH, \"rb\"))\n",
    "\n",
    "    ids = list(sybil_split_dict.keys())\n",
    "    splits = list(sybil_split_dict.values())\n",
    "    pids = [re.split('PID-|__TimePoint-|__Series-', i)[1] for i in ids]\n",
    "    timepoints = [re.split('PID-|__TimePoint-|__Series-', i)[2] for i in ids]\n",
    "    siuids = [re.split('PID-|__TimePoint-|__Series-', i)[3] for i in ids]\n",
    "\n",
    "    sybil_split_df = pd.DataFrame({'id': ids, 'split': splits, 'PatientID': pids, 'timepoint': timepoints, 'SeriesInstanceUID': siuids})\n",
    "\n",
    "    sybil_split_df.to_csv(f\"{INPUT_DIR}/sybil-nlst-info/sybil-nlst-splitinfo.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    sybil_split_df = pd.read_csv(f\"{INPUT_DIR}/sybil-nlst-info/sybil-nlst-splitinfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venk21_series_ids = set(venk21_pancan['SeriesInstanceUID'])\n",
    "print(\"Venk21 scans (cross-validation):\", len(venk21_series_ids))\n",
    "\n",
    "sybil_train_ids = set(sybil_split_df.query('split == \"train\"')['SeriesInstanceUID'])\n",
    "print(\"\\nSybil train scans:\", len(sybil_train_ids))\n",
    "print(\"Venk21 - Sybil train intersection:\", len(venk21_series_ids.intersection(sybil_train_ids)))\n",
    "\n",
    "sybil_dev_ids = set(sybil_split_df.query('split == \"dev\"')['SeriesInstanceUID'])\n",
    "print(\"\\nSybil dev scans:\", len(sybil_dev_ids))\n",
    "print(\"Venk21 - Sybil dev intersection:\", len(venk21_series_ids.intersection(sybil_dev_ids)))\n",
    "\n",
    "sybil_test_ids = set(sybil_split_df.query('split == \"test\"')['SeriesInstanceUID'])\n",
    "print(\"\\nSybil test scans:\", len(sybil_test_ids))\n",
    "print(\"Venk21 - Sybil test intersection:\", len(venk21_series_ids.intersection(sybil_test_ids)))\n",
    "\n",
    "all_sybil_ids = set(sybil_split_df['SeriesInstanceUID'])\n",
    "print(\"\\nVenk21 scans not found in Sybil:\", len(venk21_series_ids - all_sybil_ids))\n",
    "print(\"Sybil scans not found in Venk21:\", len(all_sybil_ids - venk21_series_ids))\n",
    "\n",
    "unique_series_ids = venk21_series_ids - sybil_train_ids\n",
    "print(\"\\nOUR Validation set scans (not in Sybil Train):\", len(unique_series_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venk21_pancan['InSybilTrain'] = venk21_pancan['SeriesInstanceUID'].isin(sybil_train_ids)\n",
    "venk21_pancan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sybil_infer_input = venk21_pancan.query(\"SeriesInstanceUID in @unique_series_ids\")\n",
    "sybil_infer_input.to_csv(f\"{FILE_DIR}/sybil_val_infer_series.csv\", index=False)\n",
    "sybil_infer_input[['SeriesInstanceUID']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, convert MHA to DICOM and run Sybil's inference on `DIAGNijmegen/bodyct-sybil-lung-cancer-risk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Sybil Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I ran this in two jobs - one for a subset of 1172 scans, and a second for the 4739 (1 scan failed) on the rest. Ideally, you run it all at once, but I'm gonna merge them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sybil_infer_1172 = pd.read_csv(f\"{INPUT_DIR}/sybil-inference-1172.csv\")\n",
    "sybil_infer_4739 = pd.read_csv(f\"{INPUT_DIR}/sybil-inference-4739.csv\")\n",
    "len(sybil_infer_1172), len(sybil_infer_4739)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sybil_infer = pd.concat([sybil_infer_1172, sybil_infer_4739], axis=0, ignore_index=True).drop_duplicates(subset='SeriesInstanceUID')\n",
    "sybil_infer = sybil_infer.rename(columns={f'year{n+1}': f'sybil_year{n+1}' for n in range(6)})\n",
    "sybil_infer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds = venk21_pancan.merge(sybil_infer, validate='m:1',\n",
    "                how=\"left\",\n",
    "                on=['SeriesInstanceUID'], suffixes=(None,None))\n",
    "allpreds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up Nodule Type Columns\n",
    "\n",
    "This is to make the analysis a bit easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds['NoduleType2'] = allpreds['NoduleType']\n",
    "allpreds = pd.get_dummies(allpreds, columns=['NoduleType2'], prefix='', prefix_sep='')\n",
    "allpreds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds.to_csv(f\"{FILE_DIR}/nlst_allmodels.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
