{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from utilities import predictions\n",
    "from utilities.info import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This includes the De Haas model (unused for MELBA paper). To load predictions, use these directories: `NLST_Tijmen_results`, `Tijmen_Local_NLST`, `Tijmen-Global-Hidden-NLST`, `Tijmen-Global-ShowNodule-NLST`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Calibrate DLCST Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlcst_preds = pd.read_csv(f\"{INPUT_DIR}/dlcst_allmodels.csv\")\n",
    "dlcst_preds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = predictions.check_scoredists(dlcst_preds, ['Ensemble_Kiran', 'PanCan2b', 'thijmen_mean', 'sybil_year1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['Ensemble_Kiran', 'thijmen_mean']:\n",
    "    dlcst_preds[f\"{model}_cal\"] = predictions.calibrate_preds(dlcst_preds[model], dlcst_preds['label'])\n",
    "    predictions.make_calibration_plots(\n",
    "        dlcst_preds['label'], dlcst_preds[model], dlcst_preds[f'{model}_cal'], \n",
    "        title=f'\\n{model}')\n",
    "    predictions.check_scoredists(dlcst_preds, [model, f'{model}_cal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlcst_preds.to_csv(f\"{FILE_DIR}/dlcst_allmodels_cal.csv\", index=False)\n",
    "dlcst_preds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Venk21 and PanCan Predictions (NLST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venk21_pancan = pd.read_csv(f\"{INPUT_DIR}/NLST_DL_vs_PanCan_Venk21.csv\")\n",
    "venk21_pancan.rename(columns={'Diameter [mm]': 'Diameter_mm'}, inplace=True)\n",
    "venk21_pancan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venk21_pancan['DL_cal'] = predictions.calibrate_preds(venk21_pancan['DL'], venk21_pancan['label'])\n",
    "_= predictions.make_calibration_plots(venk21_pancan['label'], venk21_pancan['DL'], venk21_pancan['DL_cal'], title='\\nVenkadesh 2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = predictions.check_scoredists(venk21_pancan, ['DL', 'DL_cal', 'PanCan2b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load De Haas Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tijmen_load_info = {\n",
    "    \"Thijmen_mean\": {\n",
    "        \"base_path\": rf\"{INPUT_DIR}/NLST_Tijmen_results/final_layer_nlst_validtion_20240626\",\n",
    "        \"local\": False,\n",
    "        \"valid\": False,\n",
    "    },\n",
    "    \"Thijmen_local\": {\n",
    "        \"base_path\": rf\"{INPUT_DIR}/Tijmen Local NLST\",\n",
    "        \"local\": True,\n",
    "        \"valid\": True,\n",
    "    },\n",
    "    \"Thijmen_global_hidden\": {\n",
    "        \"base_path\": rf\"{INPUT_DIR}/Tijmen-Global-Hidden-NLST/clip_hidden_nod_global_nlst_logits_20240422\",\n",
    "        \"local\": False,\n",
    "        \"valid\": True,\n",
    "    },\n",
    "    \"Thijmen_global_show\": {\n",
    "        \"base_path\": rf\"{INPUT_DIR}/Tijmen-Global-ShowNodule-NLST/clip_show_nod_global_nlst_20240501\",\n",
    "        \"local\": False,\n",
    "        \"valid\": True,\n",
    "    }, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tijmen_dfs = {}\n",
    "for model in tijmen_load_info:\n",
    "    df = predictions.load_tijmen_results(\n",
    "        model_name=model,\n",
    "        base_path=tijmen_load_info[model]['base_path'],\n",
    "        local=tijmen_load_info[model]['local'],\n",
    "        valid=tijmen_load_info[model]['valid'],\n",
    "    )[['AnnotationID', 'label', model]]\n",
    "\n",
    "    df[f\"{model}_cal\"] = predictions.calibrate_preds(df[model], df['label'])\n",
    "    predictions.make_calibration_plots(\n",
    "        df['label'], df[model], df[f'{model}_cal'], \n",
    "        title=f'\\n{model}')\n",
    "    predictions.check_scoredists(df, [model, f'{model}_cal'])\n",
    "\n",
    "    df.to_csv(f\"{tijmen_load_info[model]['base_path']}/combined_{('valid_' if tijmen_load_info[model]['valid'] else '')}output.csv\")\n",
    "    tijmen_dfs[model] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Venk21 and De Haas Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = venk21_pancan\n",
    "for m in tijmen_dfs:\n",
    "    preds = preds.merge(tijmen_dfs[m], how='left', on=['AnnotationID', 'label'], suffixes=(None, None))\n",
    "\n",
    "preds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Series for Sybil Predictions\n",
    "\n",
    "This includes the ones from Venk21's cross-validation set which are NOT in Sybil's training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_SYBIL_PICKLE_FILE = True\n",
    "\n",
    "if LOAD_SYBIL_PICKLE_FILE:\n",
    "    SYBIL_SERIES_JSON_PATH = f\"{INPUT_DIR}/sybil-nlst-info/sybil-nlst-pid_tp_series2split.p\"\n",
    "    sybil_split_dict = pickle.load(open(SYBIL_SERIES_JSON_PATH, \"rb\"))\n",
    "\n",
    "    ids = list(sybil_split_dict.keys())\n",
    "    splits = list(sybil_split_dict.values())\n",
    "    pids = [re.split('PID-|__TimePoint-|__Series-', i)[1] for i in ids]\n",
    "    timepoints = [re.split('PID-|__TimePoint-|__Series-', i)[2] for i in ids]\n",
    "    siuids = [re.split('PID-|__TimePoint-|__Series-', i)[3] for i in ids]\n",
    "\n",
    "    sybil_split_df = pd.DataFrame({'id': ids, 'split': splits, 'PatientID': pids, 'timepoint': timepoints, 'SeriesInstanceUID': siuids})\n",
    "\n",
    "    sybil_split_df.to_csv(f\"{INPUT_DIR}/sybil-nlst-info/sybil-nlst-splitinfo.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    sybil_split_df = pd.read_csv(f\"{INPUT_DIR}/sybil-nlst-info/sybil-nlst-splitinfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sybil_train_ids = set(sybil_split_df.query('split == \"train\"')['SeriesInstanceUID'])\n",
    "venk21_series_ids = set(preds['SeriesInstanceUID'])\n",
    "unique_series_ids = venk21_series_ids - sybil_train_ids\n",
    "\n",
    "print(\"Sybil training scans:\", len(sybil_train_ids))\n",
    "print(\"Venk21 scans (cross-val.):\", len(venk21_series_ids))\n",
    "print(\"Validation set scans:\", len(unique_series_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['InSybilTrain'] = preds['SeriesInstanceUID'].isin(sybil_train_ids)\n",
    "preds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sybil_infer_input = preds.query(\"SeriesInstanceUID in @unique_series_ids\")\n",
    "sybil_infer_input.to_csv(f\"{INPUT_DIR}/sybil-nlst-info/sybil_val_infer_series.csv\", index=False)\n",
    "sybil_infer_input[['SeriesInstanceUID']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, convert MHA to DICOM and run Sybil's inference on `DIAGNijmegen/bodyct-sybil-lung-cancer-risk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Sybil Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I ran this in two jobs - one for the 1172 scans in the intersection with De Haas combined model's validation set, and a second for the 4739 (1 scan failed) on the rest. Ideally, you run it all at once, but I'm gonna merge them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sybil_infer_1172 = pd.read_csv(f\"{INPUT_DIR}/sybil-inference-1172.csv\")\n",
    "sybil_infer_4739 = pd.read_csv(f\"{INPUT_DIR}/sybil-inference-4739.csv\")\n",
    "len(sybil_infer_1172), len(sybil_infer_4739)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sybil_infer = pd.concat([sybil_infer_1172, sybil_infer_4739], axis=0, ignore_index=True).drop_duplicates(subset='SeriesInstanceUID')\n",
    "sybil_infer = sybil_infer.rename(columns={f'year{n+1}': f'sybil_year{n+1}' for n in range(6)})\n",
    "sybil_infer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds = preds.merge(sybil_infer, validate='m:1',\n",
    "                how=\"left\",\n",
    "                on=['SeriesInstanceUID'], suffixes=(None,None))\n",
    "allpreds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up Nodule Type Columns\n",
    "\n",
    "This is to make the analysis a bit easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds['NoduleType2'] = allpreds['NoduleType']\n",
    "allpreds = pd.get_dummies(allpreds, columns=['NoduleType2'], prefix='', prefix_sep='')\n",
    "allpreds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds.to_csv(f\"{FILE_DIR}/nlst_allmodels.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
